{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torchvision import models\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the trained model checkpoint\n",
    "model = models.resnet18(pretrained=False)\n",
    "num_classes = 4  # Change this to the number of your classes\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "model.load_state_dict(torch.load('model_checkpoint.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Define the class names\n",
    "classes = ['blank', 'paper', 'rock', 'scissors']\n",
    "\n",
    "# Define the cropping and resizing transformations\n",
    "crop_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "def crop_frame(frame):\n",
    "    # Convert the frame to grayscale\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Threshold the frame to create a binary mask\n",
    "    _, thresholded_frame = cv2.threshold(gray_frame, 1, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Find contours in the binary mask\n",
    "    contours, _ = cv2.findContours(thresholded_frame, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    if contours:\n",
    "        # Find the bounding box of the largest contour\n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "        x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "        \n",
    "        # Crop the frame to the bounding box\n",
    "        cropped_frame = frame[y:y+h, x:x+w]\n",
    "        \n",
    "        return cropped_frame\n",
    "    \n",
    "    return None\n",
    "\n",
    "def process_frame(frame):\n",
    "    # Crop the frame to remove black edges\n",
    "    cropped_frame = crop_frame(frame)\n",
    "    \n",
    "    if cropped_frame is not None:\n",
    "        # Convert cropped frame to PIL Image\n",
    "        cropped_pil = Image.fromarray(cropped_frame)\n",
    "\n",
    "        # Apply image transformations\n",
    "        frame_tensor = crop_transform(cropped_pil).unsqueeze(0)\n",
    "        return frame_tensor\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    video_path = \"\" # input video path here\n",
    "    output_video_path = \"\" # place your video output path here\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    frame_width = int(cap.get(3))\n",
    "    frame_height = int(cap.get(4))\n",
    "    fps = int(cap.get(5))\n",
    "    out = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n",
    "    \n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale = 2  # Increase font size\n",
    "    font_color = (0, 255, 0)\n",
    "    text_y = 200  # Lower position on the y-axis\n",
    "\n",
    "    frame_count = 0\n",
    "    frames_to_display = 10  # Number of frames to randomly display\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frames_between_display = max(1, total_frames // frames_to_display)\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Crop the frame to remove black edges\n",
    "        cropped_frame = crop_frame(frame)\n",
    "        \n",
    "        if cropped_frame is not None:\n",
    "            # Apply cropping and resizing transformations\n",
    "            transformed_frame = crop_transform(cropped_frame)\n",
    "            transformed_frame = transformed_frame.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "            # Perform inference\n",
    "            with torch.no_grad():\n",
    "                outputs = model(transformed_frame)\n",
    "                _, predicted_class = torch.max(outputs, 1)\n",
    "                predicted_label = classes[predicted_class.item()]  # Use the predicted class index\n",
    "\n",
    "            # Calculate text size for centered text in the video\n",
    "            text_size = cv2.getTextSize(predicted_label, font, font_scale, 2)[0]\n",
    "            text_x_centered = (frame.shape[1] - text_size[0]) // 2\n",
    "\n",
    "            # Write the frame to the output video with centered text\n",
    "            cv2.putText(frame, predicted_label, (text_x_centered, text_y), font, font_scale, font_color, 2, cv2.LINE_AA)\n",
    "            \n",
    "            # Display the frame with prediction\n",
    "            if frame_count % frames_between_display == 0:\n",
    "                # Crop the frame for display using the same crop dimensions\n",
    "                cropped_for_display = crop_frame(frame)\n",
    "                if cropped_for_display is not None:\n",
    "                    plt.imshow(cropped_for_display[..., ::-1])  # Convert BGR to RGB\n",
    "                    plt.title(f\"Predicted: {predicted_label}\")\n",
    "                    plt.show()\n",
    "\n",
    "\n",
    "            out.write(frame)  # Write frame with overlay to the output video\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "    # Release the video capture and writer\n",
    "    cap.release()\n",
    "    out.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py310",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
